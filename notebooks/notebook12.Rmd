---
title: "Notebook 12: Local Models (GBM)"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "note-style.css"
---

```{r message=FALSE}
library(tidyverse)
library(forcats)
library(ggrepel)
library(smodels)
library(cleanNLP)
library(glmnet)
library(Matrix)
library(xgboost)
library(stringi)
library(magrittr)

theme_set(theme_minimal())
options(dplyr.summarise.inform = FALSE)
options(width = 77L)
options(sparse.colnames = TRUE)
```

# Amazon Authorship Data: Music

As in the previous notes, I will use the extra project dataset today:

```{r, message=FALSE}
amazon <- read_csv(file.path("data", sprintf("%s.csv.gz", "cds")))
token <- read_csv(file.path("data", sprintf("%s_token.csv.gz", "cds")))
```

We will use the same design matrix from the previous notes as well:

```{r}
X <- token %>%
  cnlp_utils_tf(doc_set = amazon$doc_id,
                min_df = 0.005,
                max_df = 1,
                max_features = 200,
                token_var = "lemma")
```

Today we will continue our study of local models by looking at a very powerful
model based on an adaptive form of KNN.

### Decision Trees

Before we get to the main model today, we need to understand an intermediate
model called a *decision tree*. Because we are not going to use these models
directly (there is no good R package for running simple decision trees), let's
look at this model using this resource:
[A visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/).
It has the benefit of giving great visual intuition for the model as well as
reviewing some keep machine learning concepts.

### Gradient Boosted Trees

An individual decision tree is often not a particularly powerful model for
complex prediction tasks. However, the idea of a tree-based model is to learn
an adaptive version of KNN. A clever way to increase the predictive power of
a decision tree is to build a large collection of trees; prediction is then
done by predicting on each individual tree and averaging (or taking the
majority class) across the whole set. One such model is called a
*random forest*. The one that we will look at here is a *gradient boosted tree*,
or *gradient boosted machine* (GBM). For a continuous response, the algorithm
works something like this:

- select a random subset of the training data
- build a decision tree with the selected training data to predict the response
variable
- take the predictions from this first tree, multiply by a fixed parameter
called the *learning rate* (say, 0.01), and compute the residuals for the
entire training set
- take another random subset of the training data
- building a decision tree with the selected training data to predict the
residuals from the first model
- repeat this process many times

If you prefer a mathematical description, if the fitted values from the t-th
tree are given by:

$$ \widehat{Y_i^t} $$

Then we train the k-th tree on the values Z given by:

$$ Z_i = Y_i - \eta \cdot \sum_{t = 1}^{k - 1} \widehat{Y_i^t} $$

The parameter eta is the learning rate. If set to one, this is exactly fitting
on the residuals of the prior trees. Setting to less than one stops the trees
from overfitting on the first few trees.

The details for classification tasks are a bit more complex, but the general
ideas are the same. To run gradient boosted trees, we will use the **xgboost**
package, which has a very fast implementation of a learning algorithm. It
requires us to convert our categorical `user_id` variable into an integer
starting at 0:

```{r}
author_set <- unique(amazon$user_id)
y <- (match(amazon$user_id, author_set) - 1L)
```

Then, we create training and validation sets, which are converted into an
efficient data structure by the function `xgb.DMatrix`.

```{r}
y_train <- y[amazon$train_id == "train"]
y_valid <- y[amazon$train_id == "valid"]
X_train <- X[amazon$train_id == "train",]
X_valid <- X[amazon$train_id == "valid",]

data_train <- xgb.DMatrix(data = X_train, label = y_train)
data_valid <- xgb.DMatrix(data = X_valid, label = y_valid)

watchlist <- list(train=data_train, valid=data_valid)
```

Then, we train the actual model using the `xgb.train` function. We set the
depth of the decision tree (here, 3), the learning rate (here, 0.05), and
the number of trees to build (here, 10). The number of threads is just a
computational details about how many cores to run on your machine. We also
have to indicate the number of classes (5) and tell xgboost that we are running
a multiclass prediction.

```{r}
model <- xgb.train(data = data_train,
                   max_depth = 3,
                   eta = 0.05,
                   nthread = 2,
                   nrounds = 10,
                   objective = "multi:softmax",
                   eval_metric = "mlogloss",
                   watchlist = watchlist,
                   verbose = TRUE,
                   num_class = length(author_set))
```

You can see that the function prints out the training and validation error
rates (1 minus the classification rate) after each step. We can do slightly
better by decreasing the learning rate and increasing the number of trees. I
will make sure that only some intermediate steps are printed out due to the
large number of trees with `print_every_n`.

```{r}
model <- xgb.train(data = data_train,
                   max_depth = 3,
                   eta = 0.03,
                   nthread = 2,
                   nrounds = 1000,
                   objective = "multi:softmax",
                   watchlist = watchlist,
                   verbose = TRUE,
                   print_every_n = 25,
                   num_class = length(author_set))
```

Let's see how well this model predicts the classes in our data:

```{r}
y_hat <- author_set[predict(model, newdata = X) + 1]

amazon %>%
  mutate(author_pred = y_hat) %>%
  group_by(train_id) %>%
  summarize(class_rate = mean(user_id == author_pred))
```

It manages about 88%, significantly better than before with the KNN model we
built in the previous section. Another useful benefit of the gradient boosted
trees over KNN is that the former also provides variable importance scores:

```{r}
importance_matrix <- xgb.importance(model = model)
importance_matrix
```

Finally, let's create a confusion matrix for our model:

```{r}
y_hat_id <- (predict(model, newdata = X) + 1)

amazon %>%
  mutate(user_num = match(user_id, author_set)) %>%
  mutate(user_num_pred = y_hat_id) %>%
  filter(train_id == "valid") %>%
  select(y = user_num, yhat = user_num_pred) %>%
  table()
```

Here we see that on the validation set, there are no commonly confused reviewers
and that the model does fairly well across all authors.

## Thoughts on local models

We will continue to make the heaviest use of regression-based models, but
the two local models we today (particularly gradient boosted trees)
will be useful to augment these, particularly when looking at features that
benefit from determining interaction terms, such as POS N-grams. In machine
learning competitions, particularly those on non-image and non-textual data,
gradient boosted trees are very often the winning model. They can, however,
take a bit of tuning to get right. Usually this consists in slowly lowering
the learning rate and increasing the number of trees until the model
*saturates* (no longer improves).
