---
title: "Notebook 11: Local Models (KNN and GBM)"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "note-style.css"
---

```{r message=FALSE}
library(tidyverse)
library(forcats)
library(ggrepel)
library(smodels)
library(cleanNLP)
library(Matrix)
library(xgboost)
library(stringi)

theme_set(theme_minimal())
options(dplyr.summarise.inform = FALSE)
options(width = 77L)
options(sparse.colnames = TRUE)
```

## Five British Authors

The data set will look at today consists of short passages from five well-known
British authors: Jane Austen, Charles Dickens, Sir Arthur Conan Doyle,
Robert Louis Stevenson, and H.G. Wells. The prediction task for this data set
is to determine the author. We start by reading in the data:

```{r, message=FALSE}
stylo <- read_csv("data/stylo_uk.csv") %>%
  mutate(train_id = if_else(runif(n()) < 0.6, "train", "valid"))
token <- read_csv("data/stylo_uk_token.csv.gz")
```

### POS N-grams

A powerful feature for learning authorship style is too look at the frequency
of part of speech n-grams (usually up to order 3 or 4). This can be done with
either the universal part of speech codes or the more granular codes in the
`xpos` variable. Let's create a data matrix with trigrams. Keep in mind that
we probably want to have `max_df = 1` here.

```{r}
X <- token %>%
  sm_ngram(n = 3, n_min = 1, token_var = "upos") %>%
  cnlp_utils_tf(doc_set = stylo$doc_id,
                min_df = 0.005,
                max_df = 1,
                max_features = 10000,
                token_var = "token")
```

And just to understand what the data set looks like, here are a few rows and
columns from the data matrix.

```{r}
X[1:10, c(1:2, 51:52, 1000:1001)]
```

We could plug this data matrix into a penalized regression model, and it would
likely perform relatively well. However, today we will use it to illustrate a
different class of models, which I call *local models*. Whereas regression tries
to learn specific weights attached to each variable, local models focus on
making predictions based on the training data most similar to the point where
one wants to make a prediction.

We will discuss three different local models and then step-back to talk about
their relative strengths and weaknesses compared to *global* models that are
similar to linear regression.

### K-nearest neighbors

Our first estimator is called K-nearest neighbors, or KNN. It is the
quintessential local model. In order to make a prediction at a point, the k
closest points in the training data are taken and a prediction is made by
averaging their response values. In the case of classification, the class
most common to the neighborhood is used (ties being broken by sequentially
removing the farthest-away training data points).

In order to run the KNN algorithm, we need to create a response vector and
training matrix:

```{r}
y <- stylo$author
y_train <- y[stylo$train_id == "train"]
X_train <- X[stylo$train_id == "train",]
```

Then, we load the package **FNN** (Fast nearest neighbors) and use the `knn`
function. We start by setting k equal to 4. Unlike linear regression, there is
no specific *model* per say. The algorithm jumps right to the predictions, which
we will save as a new vector in R.

```{r}
library(FNN)
y_hat <- knn(X_train, X, y_train, k = 4)
```

After the algorithm finishes, we can see how predictive the model is:

```{r}
stylo %>%
  mutate(author_pred = y_hat) %>%
  group_by(train_id) %>%
  summarize(class_rate = mean(author_pred == author))
```

It predicts the correct class 42% of the time. Not too bad given that there
are 5 different authors. Let's see the confusion matrix to see if there are any
authors that are particularly easy or difficult to distinguish:

```{r}
stylo %>%
  mutate(author_pred = y_hat) %>%
  select(author, author_pred, train_id) %>%
  table()
```

I don't see any particularly strong patterns here, though it does seem that
Jane Austin is a bit easier to distinguish than the other four authors.

We can modify the model by increasing the number of neighbors under
consideration. We can try 10 here, for example:

```{r}
library(FNN)
y_hat <- knn(X_train, X, y_train, k = 10)

stylo %>%
  mutate(author_pred = y_hat) %>%
  group_by(train_id) %>%
  summarize(mean(author_pred == author))
```

And we see that the classification rate improves to 47%. Note that the training
set classification rate decreases to 59%. We could experiment with k a bit more
to get the best fit, which usually comes by getting the training and validation
classification rates fairly close.


### Decision Trees

The next model I want to consider are decision trees. We won't use this directly
in this course, but they will be used as a building block for the next model
we will discuss.

Because we are not going to use these models directly (there is no good R
package for running simple decision trees), let's look at this model using this
resource:
[A visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/).
It has the benefit of giving great visual intuition for the model as well as
reviewing some keep machine learning concepts.

### Gradient Boosted Trees

An individual decision tree is often not a particularly powerful model for
complex prediction tasks. However, the idea of a tree-based model is to learn
an adaptive version of KNN. A clever way to increase the predictive power of
a decision tree is to build a large collection of trees; prediction is then
done by predicting on each individual tree and averaging (or taking the
majority class) across the whole set. One such model is called a
*random forest*. The one that we will look at here is a *gradient boosted tree*,
or *gradient boosted machine* (GBM). For a continuous response, the algorithm
works something like this:

- select a random subset of the training data
- build a decision tree with the selected training data to predict the response
variable
- take the predictions from this first tree, multiply by a fixed parameter
called the *learning rate* (say, 0.01), and compute the residuals for the
entire training set
- take another random subset of the training data
- building a decision tree with the selected training data to predict the
residuals from the first model
- repeat this process many times

If you prefer a mathematical description, if the fitted values from the t-th
tree are given by:

$$ \widehat{Y_i^t} $$

Then we train the k-th tree on the values Z given by:

$$ Z_i = Y_i - \eta \cdot \sum_{t = 1}^{k - 1} \widehat{Y_i^t} $$

The parameter eta is the learning rate. If set to one, this is exactly fitting
on the residuals of the prior trees. Setting to less than one stops the trees
from overfitting on the first few trees.

The details for classification tasks are a bit more complex, but the general
ideas are the same. To run gradient boosted trees, we will use the **xgboost**
package, which has a very fast implementation of a learning algorithm. It
requires us to convert our categorical `author` variable into an integer
starting at 0:

```{r}
author_set <- unique(stylo$author)
y <- (match(stylo$author, author_set) - 1L)
```

Then, we create training and validation sets, which are converted into an
efficient data structure by the function `xgb.DMatrix`.

```{r}
y_train <- y[stylo$train_id == "train"]
y_valid <- y[stylo$train_id == "valid"]
X_train <- X[stylo$train_id == "train",]
X_valid <- X[stylo$train_id == "valid",]

data_train <- xgb.DMatrix(data = X_train, label = y_train)
data_valid <- xgb.DMatrix(data = X_valid, label = y_valid)

watchlist <- list(train=data_train, valid=data_valid)
```

Then, we train the actual model using the `xgb.train` function. We set the
depth of the decision tree (here, 3), the learning rate (here, 0.05), and
the number of trees to build (here, 10). The number of threads is just a
computational details about how many cores to run on your machine. We also
have to indicate the number of classes (5) and tell xgboost that we are running
a multiclass prediction.

```{r}
model <- xgb.train(data = data_train,
                 max_depth = 3, eta = 0.05, nthread = 2,
                 nrounds = 10, objective = "multi:softmax",
                 watchlist = watchlist, verbose = TRUE, num_class = 5)
```

You can see that the function prints out the training and validation error
rates (1 minus the classification rate) after each step. We can do slightly
better by decreasing the learning rate and increasing the number of trees. I
will turn off (`verbose = FALSE`) the print out due to the larger number of
trees.

```{r}
model <- xgb.train(data = data_train,
                 max_depth = 3, eta = 0.03, nthread = 2,
                 nrounds = 100, objective = "multi:softmax",
                 watchlist = watchlist, verbose = FALSE, num_class = 5)
```

Let's see how well this model predicts the classes in our data:

```{r}
y_hat <- author_set[predict(model, newdata = X) + 1]

stylo %>%
  mutate(author_pred = y_hat) %>%
  group_by(train_id) %>%
  summarize(mean(author_pred == author))
```

It manages about 54%, better than before and better than the KNN model we built
in the previous section. One useful benefit of the gradient boosted trees over
KNN is that the former also provides variable importance scores:

```{r}
importance_matrix <- xgb.importance(model = model)
importance_matrix
```

Finally, let's create a confusion matrix for our model:

```{r}
y_hat <- author_set[predict(model, newdata = X) + 1]

stylo %>%
  mutate(pred = y_hat) %>%
  select(author, pred, train_id) %>%
  table()
```

Here we see that on the validation set, the gradient boosted trees have done
a better job classifying Jane Austen and H.G. Wells (around 800 correct
classifications compared to 600 with KNN) relative to the other three.

## Thoughts on local models

We will continue to make the heaviest use of regression-based models, but
the local models we covered today (particularly gradient boosted trees)
will be useful to augment these, particularly when looking at features that
benefit from determining interaction terms, such as POS N-grams. In machine
learning competitions, particularly those on non-image and non-textual data,
gradient boosted trees are very often the winning model. They can, however,
take a bit of tuning to get right. Usually this consists in slowly lowering
the learning rate and increasing the number of trees until the model
*saturates* (no longer improves).

## Standardizing the training matrix

One last thing to note today is that when working with features such as POS
tags, particularly if the input texts are different lengths, it can be useful
to try to standardize the TF matrix so that sum of each row is the same. This
can be done with the following code:

```{r}
X <- X / apply(X, 1, sum)
```

The counts now become proportions rather than raw counts:

```{r}
X[1:10, c(1:2, 51:52, 1000:1001)]
```

This will not always produce better estimates, but it is a useful feature to
consider in authorship models.
