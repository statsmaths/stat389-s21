---
title: "Notebook 14: Example Third Projects - Montréal and Urbana-Champaign"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "note-style.css"
---

```{r message=FALSE, include=FALSE}
library(tidyverse)
library(forcats)
library(ggrepel)
library(smodels)
library(cleanNLP)
library(Matrix)
library(magrittr)
library(stringi)
library(glmnet)

theme_set(theme_minimal())
options(dplyr.summarise.inform = FALSE)
options(width = 77L)
```

I thought it would be useful to show a few examples of things that could be the
focus of a presentation for Project 3, using two different cities: Montréal and
Urbana-Champaign. Urbana-Champaign is quite a bit smaller than the projects you
have in class and I think is more difficult to find something interesting.
Montréal on the other hand is significantly more complex due to the use of
French and English in the reviews; there is a lot there, but I thought it may
be too complex given the short time line.

Each of the three sections below are a good guide to the
number of tables and plots individual project probably should be. Yes, perhaps
you would add a few more examples, but use the sections as a guide for how large
a scope you should be trying to capture with your work.

The three examples cover the following topics:

1. The use of French and English in Montréal reviews.
2. The effect of location in the Urbana-Champaign reviews.
3. Gender in reviews from Urbana-Champaign.

Note that I tried to focus on thing that extend what I had already done in the
notes to show what is possible. If your project focuses more on the methods in
Notebook 13, that will not be a problem. However, if you want to do something
a bit different, I of course encourage that as well!

# 1. Montreal/Montréal: French and English

I will start by reading in the yelp data for Montréal:

```{r, message=FALSE}
yelp <- read_csv("data/montreal.csv.gz")
token <- read_csv("data/montreal_token.csv.gz")
```

And then, store and plot the three clusters that appear in the PCA components
of the TF matrix:

```{r, warning = FALSE}
set.seed(5)
clusters <- token %>%
  cnlp_utils_tf(
    doc_set = unique(yelp$user_name),
    min_df = 0.001,
    max_df = 1.0,
    max_features = 10000,
    doc_var = "user_name",
    token_var = "token"
  ) %>%
  sm_tidy_pca(n = 2) %>%
  sm_kmeans(clusters = 3)

clusters %>%
  ggplot(aes(x = v1, y = v2)) +
    geom_point(aes(color = factor(cluster))) +
    geom_text_repel(
      aes(label = document),
      show.legend = FALSE,
      max.overlaps = 20,
      size = 2
    ) +
    labs(color = "Cluster Number") +
    scale_color_viridis_d() +
    theme_void()
```

Notice that there is a strong clustering here (you likely do not have anything
as strong in the cities that I gave to use for class). Looking at the TF-IDF
terms as a function of cluster sheds some light on what is going on:

```{r}
token %>%
  filter(upos == "NOUN") %>%
  sm_text_tfidf(doc_var = "user_name", token_var = "lemma") %>%
  group_by(doc_id) %>%
  arrange(desc(tfidf)) %>%
  slice_head(n = 5) %>%
  summarize(tokens = paste(token, collapse = "; ")) %>%
  left_join(select(clusters, document, cluster), by = c("doc_id" = "document")) %>%
  arrange(cluster) %>%
  print.data.frame()
```

The reviews in cluster 2 are in French and those in cluster 3 are in English.
Cluster 1 consists of only two people; the top TF-IDF words are in French,
but what is actually going on? Looking at some example texts, Hugo kindly
provides his reviews in both languages:

```{r}
set.seed(1)
yelp %>%
  filter(user_name == "Hugo") %>%
  sample_n(size = 4) %>%
  use_series(text)
```

Whereas Mathieu switches between reviews (it's not clear why, but would be a
good deep dive to look into); in one case he also translates the review, but
this is not as common.

```{r}
set.seed(1)
yelp %>%
  filter(user_name == "Mathieu") %>%
  sample_n(size = 4) %>%
  use_series(text)
```

Using the clusters we can see the relative popularity of each business type
based on the clusters. Brasseries and cafés are particularly popular with
Francophone reviewers; Burgers, Greek, and Chinese are popular with Anglophone
reviewers. Interestingly, there are a lot of reviews for French restaurants,
but these are evenly distributed between the two groups.

```{r}
yelp %>%
  left_join(clusters, by = c("user_name" = "document")) %>%
  filter(biz_category != "Other") %>%
  group_by(biz_category, cluster) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  pivot_wider(
    id_cols = c(biz_category),
    names_from = cluster,
    values_from = n,
    names_prefix = "cluster_",
    names_sort = TRUE
  ) %>%
  mutate(
    cluster_2 = cluster_2 / sum(cluster_2) * 100,
    cluster_3 = cluster_3 / sum(cluster_3) * 100
  ) %>%
  ggplot(aes(cluster_2, cluster_3)) +
    geom_abline(alpha = 0.5) +
    geom_point() +
    geom_text_repel(aes(label = biz_category)) +
    labs(x = "Francophone Reviewers (%)", y = "Anglophone Reviewers (%)")
```

Of course, you likely won't have as strong a single in your dataset. But this
would not stop you from clustering in a similar way and doing a similar
analysis.

# 2. Location in Urbana-Champaign

As a second example, let's grab the Yelp data from Urbana-Champaign.

```{r, message=FALSE}
yelp <- read_csv("data/urbana-champaign.csv.gz")
token <- read_csv("data/urbana-champaign_token.csv.gz")
```

We will try to look at the spatial distribution of the data, which we can
start doing by running a KMeans clustering algorithm on the longitude and
latitude variables.

```{r}
set.seed(5)
clusters <- yelp %>%
  select(doc_id, lon, lat) %>%
  sm_kmeans(clusters = 20, item_name = "doc_id") %>%
  select(-lon, -lat)

yelp %>%
  left_join(clusters, by = "doc_id") %>%
  ggplot(aes(lon, lat)) +
    geom_point(aes(color = factor(cluster)), show.legend = FALSE) +
    labs(x = "Longitude", y = "Latitude", color = "Cluster ID")
```

Not super interesting on its own, but we can use these clusters as grouping
variables. Several of these are interesting:

```{r}
token %>%
  filter(upos == "NOUN") %>%
  left_join(clusters, by = "doc_id") %>%
  sm_text_tfidf(doc_var = "cluster", token_var = "lemma") %>%
  group_by(doc_id) %>%
  arrange(desc(tfidf)) %>%
  slice_head(n = 8) %>%
  summarize(tokens = paste(token, collapse = "; ")) %>%
  print.data.frame()
```

One that caught my eye was cluster 14, as it seems to be related to businesses
near or related to a University. Let's plot this cluster specifically over a
map; it shows that it is in fact located at the University of Illinois.

```{r, message=FALSE}
library(ggmaptile)

yelp %>%
  left_join(clusters, by = "doc_id") %>%
  filter(cluster == 14) %>%
  ggplot(aes(lon, lat)) +
    stat_maptiles(aspect_ratio = 1, zoom = 15) +
    geom_point(color = "orange")
```

With this cluster, let's see who tends to review locations in this area:

```{r}
count_cluster_14 <- yelp %>%
  left_join(clusters, by = "doc_id") %>%
  filter(cluster == 14) %>%
  group_by(user_name) %>%
  summarize(n = n()) %>%
  arrange(desc(n))

count_cluster_14 %>%
  slice_head(n = 15) %>%
  mutate(user_name = fct_inorder(user_name)) %>%
  ggplot(aes(fct_rev(user_name), n)) +
    geom_col(color = "black", fill = "grey") +
    coord_flip() +
    labs(x = "User Name", y = "Number of Reviews in University Cluster")
```

Now we can see the tokens these use in these reviews. Do you think these were
written by college students?

```{r}
token %>%
  filter(upos == "NOUN") %>%
  semi_join(filter(clusters, cluster == 14), by = "doc_id") %>%
  sm_text_tfidf(doc_var = "user_name", token_var = "lemma") %>%
  group_by(doc_id) %>%
  arrange(desc(tfidf)) %>%
  slice_head(n = 8) %>%
  summarize(tokens = paste(token, collapse = "; ")) %>%
  left_join(count_cluster_14, by = c("doc_id" = "user_name")) %>%
  filter(n > 6) %>%
  arrange(desc(n)) %>%
  print.data.frame()
```

Urbana-Champaign is quite small compared to the other cities in the collection.
My guess is that a similar analysis would turn up even more interesting clusters
in larger cities such as Toronto and Las Vegas.

# 3. Predicted User Gender in Urbana-Champaign

As a final example, let's look at the predicted gender tags. To start, we will
actually use some supervised learning but building a model with all lemmas that
do not contain capital letters or non-alphanumeric values:

```{r}
X <- token %>%
  filter(!stri_detect(lemma, regex = "[A-Z\\W]")) %>%
  cnlp_utils_tf(
    doc_set = yelp$doc_id,
    min_df = 0.001,
    max_df = 1.0,
    max_features = 10000,
    doc_var = "doc_id",
    token_var = "lemma"
  )

X_train <- X[yelp$train_id == "train", ]
y_train <- yelp$gender[yelp$train_id == "train"]

model <- cv.glmnet(
  X_train,
  y_train,
  alpha = 0.9,
  family = "multinomial",
  nfolds = 3,
  trace.it = FALSE,
  relax = FALSE,
  lambda.min.ratio = 0.01,
  nlambda = 100
)
```

The classification rate is relatively low given that there are only two
categories:

```{r}
yelp %>%
  mutate(pred = as.vector(predict(model, newx = X, type = "class"))) %>%
  group_by(train_id) %>%
  summarize(class_rate = mean(gender == pred))
```

The most predictive words, however, are interesting:

```{r}
temp <- coef(model, s = model$lambda[14])
beta <- Reduce(cbind, temp)
beta <- beta[apply(beta != 0, 1, any),]
colnames(beta) <- names(temp)
beta
```

Now, how can we use this information in an unsupervised fashion. Let's start by
building a data set showing how often an individual's reviews are classified
as "female":

```{r}
class_rate <- yelp %>%
  mutate(pred = as.vector(predict(model, newx = X, type = "class"))) %>%
  mutate(prop = as.vector(
    predict(model, newx = X, type = "response")[,,1][,1]
  )) %>%
  group_by(user_name) %>%
  summarize(
    class_rate = mean(gender == pred),
    gender = first(gender),
    perc_female = mean(prop)
  ) %>%
  arrange(perc_female)

class_rate
```

Now, we can use this information provided by a predictive model to help
better understand the data using unsupervised tools. For example, what are
the top TF-IDF terms for each user, orderd by the average percentage that their
reviews are assigned to the "female" category:

```{r}
token %>%
  filter(upos == "NOUN") %>%
  sm_text_tfidf(doc_var = "user_name", token_var = "lemma") %>%
  group_by(doc_id) %>%
  arrange(desc(tfidf)) %>%
  slice_head(n = 3) %>%      # setting small to show the results
  summarize(tokens = paste(token, collapse = "; ")) %>%
  left_join(
    select(class_rate, user_name, perc_female, gender),
    by = c("doc_id" = "user_name")
  ) %>%
  arrange(perc_female) %>%
  print.data.frame()
```

We can also look at the relationship between the predicted gender of reviews
for each business based on our text analysis relative to the proportion based
on the predicted gender based on the name.

```{r, warning=FALSE}
yelp %>%
  mutate(pred = as.vector(predict(model, newx = X, type = "class"))) %>%
  group_by(b_name) %>%
  summarize(prop_female = mean(gender == "female"), prop_female_pred = mean(pred == "female")) %>%
  ggplot(aes(prop_female, prop_female_pred)) +
    geom_point(size = 0.8) +
    geom_text_repel(aes(label = b_name), size = 2) +
    labs(x = "Proportion of Reviews Written by Users Predicted to be Female",
         y = "Average Predicated Pobability Review Written by Female")
```

A lot of women seem to leave reviews at Walmart, but leave relatively masculine
reviews when they do. Miga (a sushi restaurant) is the other way around. A lot
reviews are left by users tagged as "male", but the model thinks they contain a
lot of feminine language.

The Urbana-Champaign dataset is relatively small. My guess is that you would
find some more interesting effects looking at gender on one of the class
data sets.
