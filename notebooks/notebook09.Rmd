---
title: "Notebook 09: More Model Features"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "note-style.css"
---

```{r message=FALSE}
library(tidyverse)
library(forcats)
library(ggrepel)
library(smodels)
library(cleanNLP)
library(glmnet)
library(magrittr)
library(stringi)

theme_set(theme_minimal())
options(dplyr.summarise.inform = FALSE)
options(width = 77L)
options(sparse.colnames = TRUE)
```

## Amazon Products

As a good example dataset, let's again use the Amazon product classification
data:

```{r, message=FALSE}
set.seed(1)

amazon <- read_csv("data/amazon_product_class.csv") %>%
  mutate(train_id = if_else(runif(n()) < 0.6, "train", "valid"))
token <- read_csv("data/amazon_product_class_token.csv.gz")
```

Today we are going to discuss additional ways to construct your model matrix,
building off of the techniques in the previous notebook. In the Notebook 8
pipeline, these will replace or augment Step 2. Note that many of these will
be useful only in a certain set of applications. It is generally good to start
with raw frequencies and move to other options given the specific goals you
have and the data you are working with.

### Adding Pronouns

The spaCy NLP pipeline converts all pronouns into a single lemma: "-PRON-". I
have always found this strange, and it is often not great for the types of
analysis we are doing with the lemmas. To fix this, we can replace every pronoun
lemma with its lower case token. Looking at the first few columns of the data
matrix, we see that some of the pronouns are quite common:

```{r}
X <- token %>%
  mutate(lemma = if_else(upos == "PRON", stri_trans_tolower(token), lemma)) %>%
  filter(upos %in% c("ADJ", "ADV", "NOUN", "VERB", "PRON")) %>%
  cnlp_utils_tf(
    doc_set = amazon$doc_id,
    min_df = 0.001,
    max_df = 1.0,
    max_features = 10000,
    doc_var = "doc_id",
    token_var = "lemma"
  )

X[1:10, 1:15]
```

Using this in your model may improve the predictive power and show different
writing strategies used in each category.

### Sentence Number

Another technique is to restrict the sentences used in creating the term
frequency matrix. This code, for example, will look at just the first sentence
in each document.

```{r}
X <- token %>%
  filter(sid == 1) %>%
  filter(upos %in% c("ADJ", "ADV", "NOUN", "VERB")) %>%
  cnlp_utils_tf(
    doc_set = amazon$doc_id,
    min_df = 0.001,
    max_df = 1.0,
    max_features = 10000,
    doc_var = "doc_id",
    token_var = "lemma"
  )
```

I find looking at the first 1 or 2 sentences can be useful when working with
reviews. Often the first sentence gives the clearest signal of someone's
sentiment towards a product, which is then clarified and refined later in the
body of the review.

With a slightly different approach, we can also look at the last sentence in
each document:

```{r}
X <- token %>%
  group_by(doc_id) %>%
  filter(sid == max(sid)) %>%
  filter(upos %in% c("ADJ", "ADV", "NOUN", "VERB")) %>%
  cnlp_utils_tf(
    doc_set = amazon$doc_id,
    min_df = 0.001,
    max_df = 1.0,
    max_features = 10000,
    doc_var = "doc_id",
    token_var = "lemma"
  )
```

This is often useful for looking at writing style, as particular people tend
to end their reviews with similar language.

### POS as a Feature

The code we have so far can be easily changed to look at the frequency of
linguistic features other than lemmas. For example, we could look at how often
each part of speech is used in the reviews by changing the `token_var`
component in the `cnlp_utils_tf` function.

```{r}
X <- token %>%
  cnlp_utils_tf(
    doc_set = amazon$doc_id,
    min_df = 0.001,
    max_df = 1.0,
    max_features = 10000,
    doc_var = "doc_id",
    token_var = "upos",

  )

X[1:10, 1:10]
```

With part of speech codes, it is often also useful to look at relatively large
N-grams. As there are fewer options for the part of speech codes, it is not
unreasonable to look at 4- or even 5-grams.

```{r}
X <- token %>%
  sm_ngram(n = 4, n_min = 1, doc_var = "doc_id", token_var = "upos") %>%
  cnlp_utils_tf(
    doc_set = amazon$doc_id,
    min_df = 0.001,
    max_df = 1.0,
    max_features = 10000,
    doc_var = "doc_id",
    token_var = "token"
  )

dim(X)
```

Looking at POS N-grams is usually most useful to isolating writing style, and
is a good way of distinguishing one author from another.

### Tagging Lemmas with POS

Often a single lemma can be the same for different parts of speech, such as
the noun "love" and the verb "love". We can treat these as different by
combining the lemma and the part of speech together into a single variable
and using this as the thing we are counting in the data matrix.

```{r}
X <- token %>%
  mutate(lemma = if_else(upos == "PRON", stri_trans_tolower(token), lemma)) %>%
  mutate(lemma = sprintf("%s_%s", lemma, upos)) %>%
  filter(upos %in% c("ADJ", "ADV", "NOUN", "VERB", "PRON")) %>%
  cnlp_utils_tf(
    doc_set = amazon$doc_id,
    min_df = 0.001,
    max_df = 1.0,
    max_features = 10000,
    doc_var = "doc_id",
    token_var = "lemma"
  )

X[1:10, 1:8]
```

I rarely find that this makes a significant difference in the predictiveness of
the model, but it does help interpret the coefficients by understanding the
correct meaning of the lemma. For example, above we know that "will" is the
modal verb ("I will be late") and not the noun ("He had the will to carry on").

### Scaling the Counts

Another way to modify the features in your data is to change how the term
frequencies are scaled. By default, we just use raw counts, but can change the
option `tf_weight` to "binary" to convert every word into a 0/1 indicator
variable:

```{r}
X <- token %>%
  filter(upos %in% c("ADJ", "ADV", "NOUN", "VERB")) %>%
  cnlp_utils_tf(
    doc_set = amazon$doc_id,
    min_df = 0.001,
    max_df = 1.0,
    max_features = 10000,
    doc_var = "doc_id",
    token_var = "lemma",
    tf_weight = "binary"
  )

X[1:10, 1:15]
```

This can be useful to compare texts of different lengths and to avoid focusing
the model on very common words (without the need to a hard cut-off).

### Manual Features from Tokens

Another way to modify the available features for analysis is to construct a
set of individual features from the token table. For example, we can compute
the type token ratio (number of unique words divided by the number of all
words), the number of sentences, and the number of words in each document.
I will also compute the frequency of the lemma "that" just to show how this
can be compared to the TF matrix.

```{r}
token_summary <- token %>%
  group_by(doc_id) %>%
  filter(!is.na(lemma)) %>%
  summarize(
    type_token_ratio = length(unique(lemma)) / n(),
    num_sentences = max(sid),
    num_tokens = n(),
    num_word_that = sum(lemma == "that")
  )
```

One thing that is nice about this data is that we can visualize it on its own,
without needing a predictive model:

```{r, message=FALSE}
amazon %>%
  inner_join(token_summary, by = "doc_id") %>%
  group_by(category) %>%
  summarize(sm_mean_ci_normal(type_token_ratio)) %>%
  arrange(desc(type_token_ratio_mean)) %>%
  ggplot() +
    geom_pointrange(aes(
      x = category,
      y = type_token_ratio_mean,
      ymin = type_token_ratio_ci_min,
      ymax = type_token_ratio_ci_max
    ))
```

Of course, we can also put this into a model matrix:

```{r}
X_cov <- amazon %>%
  select(doc_id, category) %>%
  left_join(token_summary, by = "doc_id") %>%
  model.frame(category ~ type_token_ratio + num_sentences + num_tokens -1, data = .) %>%
  model.matrix(attr(., "terms"), .)

head(X_cov)
```

And use in alone or along-side other predictors.

### Word Frequency

The final new feature type we will see today uses a new dataset that is
included anytime you load the **cleanNLP** package. It is called
`word_frequency`, and give the frequency of different words over a large
corpus of website texts:

```{r}
word_frequency
```

We can use this to remove very common words from our TF matrix as follows:

```{r}
X <- token %>%
  mutate(lemma = if_else(upos == "PRON", stri_trans_tolower(token), lemma)) %>%
  filter(upos %in% c("ADJ", "ADV", "NOUN", "VERB", "PRON")) %>%
  semi_join(
    filter(word_frequency, frequency < 0.001), by = c("lemma" = "word")
  ) %>%
  cnlp_utils_tf(
    doc_set = amazon$doc_id,
    min_df = 0.001,
    max_df = 1.0,
    max_features = 10000,
    doc_var = "doc_id",
    token_var = "lemma"
  )

X[1:10, 1:11]
```

Another, somewhat more interesting, usage of word frequencies is to use them
as a meta-feature in understanding word usage within different documents.
To start, let's break the word frequencies into 10 buckets according to their
overall frequency (so in theory, each bucket will occur in roughly the same
proportion):

```{r}
ngroups <- 10
freq_b <- word_frequency %>%
  mutate(cum_freq = cumsum(frequency)) %>%
  mutate(bucket = cut(
    cum_freq,
    breaks = seq(0, 100, length.out = ngroups + 1),
    labels = FALSE,
    include.lowest = TRUE
  ))
freq_b
```

Then, we can assign each lemma to one of the frequencies and count how often
each bucket is used in each document:

```{r}
token_summary <- token %>%
  inner_join(freq_b, by = c("lemma" = "word")) %>%
  group_by(doc_id, bucket) %>%
  summarize(sm_count()) %>%
  mutate(count = count / sum(count) * 100) %>%
  ungroup() %>%
  pivot_wider(
    names_from = "bucket",
    names_prefix = "bucket_",
    values_from = "count",
    names_sort = TRUE,
    values_fill = 0
  )

token_summary
```

And then this can be used as a set of features:

```{r}
X_cov <- amazon %>%
  select(doc_id, category) %>%
  left_join(token_summary, by = "doc_id") %>%
  select(-doc_id) %>%
  model.frame(category ~ . -1, data = .) %>%
  model.matrix(attr(., "terms"), .)

X_cov[1:10, 1:6]
```

You can change the number of buckets, or collapse some of the most frequent
ones. These are often useful features to use when looking at writing style.

### Extra: Variable Importance

```{r}
X <- token %>%
  filter(upos %in% c("ADJ", "ADV", "NOUN", "VERB")) %>%
  cnlp_utils_tf(
    doc_set = amazon$doc_id,
    min_df = 0.001,
    max_df = 1.0,
    max_features = 10000,
    doc_var = "doc_id",
    token_var = "lemma"
  )

X_train <- X[amazon$train_id == "train", ]
y_train <- amazon$category[amazon$train_id == "train"]

model <- cv.glmnet(
  X_train,
  y_train,
  alpha = 0.9,
  family = "multinomial",
  nfolds = 3,
  trace.it = FALSE,
  relax = FALSE,
  lambda.min.ratio = 0.01,
  nlambda = 100
)
```

```{r}
beta <- coef(model, s = model$lambda)
imp <- lapply(beta, function(v) apply(abs(sign(v)), 1, sum) )
imp <- Reduce(cbind, imp)[-1,]
imp <- tibble(token = rownames(imp), importance = apply(imp, 1, sum))
imp <- arrange(imp, desc(importance))
imp
```

