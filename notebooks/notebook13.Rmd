---
title: "Notebook 13: Unsupervised Learning"
output:
  html_document:
    theme: cosmo
    highlight: zenburn
    css: "note-style.css"
---

```{r message=FALSE, include=FALSE}
library(tidyverse)
library(forcats)
library(ggrepel)
library(smodels)
library(cleanNLP)
library(Matrix)
library(magrittr)

theme_set(theme_minimal())
options(dplyr.summarise.inform = FALSE)
options(width = 77L)
```

Today we make a shift towards a different type of analysis. So far we
have been mostly interested in a large collection of texts with the goal of
associating features in predicting some response variable. Now, we start to
consider the case where we are interested in understanding specific documents
in a corpus. We will serve two purposes today by introducing some new methods
while also introducing the data format for the third project.

## Yelp Dataset

The third project works with a collection of reviews from the website Yelp. It
is similar to the Amazon produce reviews, but contains significantly more
metadata and more authors. I have also a bit cleaner than the Amazon data.
Each group has been given a different city of data. In the notes I will look
at data from Toronto.

Let's load the dataset into R and look at the full data table:

```{r, message=FALSE}
yelp <- read_csv("data/toronto.csv.gz")
token <- read_csv("data/toronto_token.csv.gz")

yelp
```

You will see that, in addition to the user name and user id (note, these are
uniquely defined; you can use either one), there is also other information.
For example, we have a predicted gender for the reviewer, the number of stars
the review was given, the business name, the business category, and the latitude
and longitude of the business. Note that the variable `b_name` collapses less
common businesses into an "Other" category.

The tokens data set contains the same columns as in our other datasets; I have
also put several important variables directly in the tokens table to make them
easy to work with.

```{r}
token
```

You may already have some ideas about what we might do with this dataset.
Perhaps predicting the number of stars in the review? Or the user name? We could
even predict the gender of the reviewer or the category of the business.
And yes,  you are welcome to do all of these things.

Today, however, I want to also show another
entirely different set of techniques. Rather than treating these covariates as
*supervising* variables that we need to predict, we will collapse all of the
reviews with a common indentifier and use techniques to understand the structure
of each of these meta-documents. So, to summarize, for us:

- **supervised learner**: Small set of categories with many examples of each.
We want to find trends to predict the category based on common properties of
the examples.
- **unsupervised learning**: We are interested in each document individually,
to understand how it relates to all of the other documents. We will usually
construct these by collapsing documents across a supervised variable (i.e.,
combine all of the reviews from a single person into one).

In the notes, I will work with the business categories. For the project, you
will also look at the other variables.

## An Illustration

To illustrate the techniques that we are going to see, let's do a quick
illustration. We will call the function `cnlp_utils_tfidf()` using the variable
`biz_category` in place of `doc_id` to create a matrix with just one row for
each business category:

```{r}
X <- token %>%
  cnlp_utils_tf(
    doc_set = unique(yelp$biz_category),
    min_df = 0.001,
    max_df = 1.0,
    max_features = 10000,
    doc_var = "biz_category",
    token_var = "token"
  )

dim(X)
```

Now, let's plot two of the columns, the two corresponding to the terms 'Asian'
and 'beer':

```{r}
# NOTE: This is just for illustration; probably do not want to use it
tibble(
  biz_category = rownames(X),
  asian = as.numeric(X[,"Asian"]),
  beer = as.numeric(X[,"beer"])
) %>%
  filter(biz_category != "Other") %>%
  ggplot(aes(asian, beer)) +
    geom_point(color = "grey85") +
    geom_text_repel(aes(label = biz_category), max.overlaps = 40) +
    scale_x_continuous(limits = c(-1, NA)) +
    scale_y_continuous(limits = c(-1, NA))
```

Notice how, even with just these two terms, we can begin to see the
relationships between each of the business types. Do you see some patterns here
that follow your intuition about these businesses?

Of course, we will eventually want to understand the data using all of the
columns of X. That's where we need to learn a few new techniques.

## TF-IDF

In this course we have done a lot of work using the term frequency (TF) matrix.
As we move into unsupervised learning, we will see that it is important to
modify this object to scale the entries to account for the overall frequency
of terms across the corpus. To do this, we will use a TF-IDF (term
frequency-inverse document frequency) matrix. Mathematically, if
`tf` are the number of times a term is used in a document, `df` are the
number of documents that use the term at least once, and `N` are the total
number of document, the TF-IDF score can be computed as:

$$ \text{tfidf} = (1 + log_2(\text{tf})) \times log_2(\text{N} / \text{df}) $$

The score gives a measurement of how important a term is in describing a
document in the context of the other documents. Note that this is a popular
choice for the scaling functions, but they are not universal and other choices
are possible. We can create the TF-IDF matrix by replacing the normal function
with `cnlp_utils_tfidf`.

In addition to its other uses, which we will see below, we can also TF-IDF to
try to measure the most important words in each document by finding the terms
that have the highest score. For this one particular case, we will use the
function `sm_text_tfidf` (it returns a data frame with three columns in a
'long' format, rather than a matrix). Let's see how it works in this case:

```{r, warning = FALSE}
token %>%
  sm_text_tfidf(doc_var = "biz_category", token_var = "lemma") %>%
  group_by(doc_id) %>%
  arrange(desc(tfidf)) %>%
  slice_head(n = 5) %>%
  summarize(tokens = paste(token, collapse = "; ")) %>%
  print.data.frame()
```

And again, how well does this match your intuition?

## Distances

One way to understand the structure of our data in the high-dimensional space
of the TF-IDF matrix is to compute the distance between documents. We could do
this with typical Euclidean distances. However, this would by heavily biased
based on the number of words in each document. A better approach is to look
at the angle between two documents. Look at the two word example above to get
some intuition for why.

We can compute angle distances that with the function
`sm_tidy_angle_distance()`:

```{r}
token %>%
  cnlp_utils_tfidf(doc_var = "biz_category", token_var = "lemma") %>%
  sm_tidy_angle_distance()
```

After removing self-similarities, let's see what the closest neighbor is to
each document:

```{r}
token %>%
  cnlp_utils_tfidf(doc_var = "biz_category", token_var = "lemma") %>%
  sm_tidy_angle_distance() %>%
  filter(document1 < document2) %>%
  group_by(document1) %>%
  arrange(distance) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  arrange(distance)
```

Do you see some relationships that suggest this approach is reasonable?

## Principal component analysis (PCA)

Principal component analysis is a common method for taking a high-dimensional
data set and converting it into a smaller set of dimensions that capture many
of the most interesting aspects of the higher dimensional space. The first
principal components is defined as a direction in the high-dimensional space
that captures the most variation in the inputs. The second component is a
dimension perpendicular to the first that captures the highest amount of
residual variance. Additional components are defined similarly.

If you prefer a mathematical definition, define the following vector (called
the loading vector) of the data matrix X:

$$ W_1 = \text{argmax}_{\; v : || v ||_2 = 1} \left\{ || X v ||_2 \right\} $$

Then, the first principal component is given by:

$$ Z_1 = X \cdot W_1 $$
The second loading vector (W2) is defined just as W1, but with the argmax taken
over all unit vectors perpendicular to W1. And so on.

We can compute principal components using the helper function `sm_tidy_pca`.
We will grab the first 4 components here:

```{r, warning = FALSE}
token %>%
  cnlp_utils_tfidf(
    doc_set = unique(yelp$biz_category),
    min_df = 0.001,
    max_df = 1.0,
    max_features = 10000,
    doc_var = "biz_category",
    token_var = "token"
  ) %>%
  sm_tidy_pca(n = 4)
```

What can we do with these components? Well, for one thing, we can plot the
first two components to show the relationship between our documents within
the high dimensional space:

```{r, warning = FALSE}
token %>%
  cnlp_utils_tf(
    doc_set = unique(yelp$biz_category),
    min_df = 0.001,
    max_df = 1.0,
    max_features = 10000,
    doc_var = "biz_category",
    token_var = "token"
  ) %>%
  sm_tidy_pca() %>%
  ggplot(aes(x = v1, y = v2)) +
    geom_point(color = "grey90") +
    geom_text_repel(
      aes(label = document),
      show.legend = FALSE
    ) +
    theme_void()
```

I will keep asking this: does the relationship amongst the points make sense
and does it tell us anything interesting?

## UMAP

A similar, slightly more complex, method for doing dimensionality reduction
is called Uniform Manifold Approximation and Projection (UMAP). I will
refer you to the [original paper](https://arxiv.org/abs/1802.03426) for a
formal description; in short, it tries to preserve the distances between points
while spreading them out in a lower dimension. We can compute the projection
using `sm_tidy_umap()`

```{r, warning = FALSE}
X %>%
  sm_tidy_umap() %>%
  ggplot(aes(x = v1, y = v2)) +
    geom_point(color = "grey90") +
    geom_text_repel(
      aes(label = document),
      show.legend = FALSE
    ) +
    theme_void()
```

As with the principal components, the exact value are unimportant here,
its the relationship between the documents that counts. Notice that the pages
are less clumped together here, but also that the structures from the principal
component analysis are not as clearly defined. The benefits of UMAP become more
apparent with larger datasets.

## Clusters with K-means

One thing that we often do when looking at plots of dimensionality reduction
is to look for clumps of documents that co-occur. We can do this explicit
by clustering the data using a clustering algorithm. Here, we will use a popular
option called K-means (note, this is not the same as k-NN, but I suppose it is
not entirely unrelated). It is an iterative algorithm that works as follows:

1. Pick the number of clusters N that you want to detect.
2. Randomly choose N data points as the starting centers of each cluster.
3. Compute the distance of every data point to the centers of the current
clusters.
4. Assign each data point to the cluster whose center it is closest.
5. Re-compute the cluster centers as the average value of all the points in a
cluster.
6. Take the new cluster centers, and repeat the process (computer distances,
reassign to groups, and recompute the centers) iteratively until convergence.

The algorithm is not entirely deterministic because of the random starting
points. Typically, the algorithm is run several times and the "best" clustering
is chosen. How do we define the "best" in the case of a clustering algorithm?
A typical method is to measure the sum of squared distances to the cluster
centers, a quantity that a good clustering will minimize.

We can implement the K-means algorithm using the function `sm_kmeans`. Here we
will run it in just the first two principal components and 5 clusters:

```{r}
token %>%
  cnlp_utils_tfidf(doc_var = "biz_category", token_var = "lemma") %>%
  sm_tidy_pca(n = 2) %>%
  sm_kmeans(clusters = 5) %>%
  ggplot(aes(v1, v2)) +
    geom_point(aes(color = factor(cluster))) +
    scale_color_viridis_d()
```

Often you will find that including a few dozen PCA components provides
a better fit (though it is rarely useful to use the entire TF-IDF). We can
plot this clustering as well, but note that the clusters are harder to see
because they are formed in 10-dimensions but plotted in 2. For these, then,
it is usually easier to look at the clusters as text:

```{r}
token %>%
  cnlp_utils_tfidf(doc_var = "biz_category", token_var = "lemma") %>%
  sm_tidy_pca(n = 10) %>%
  sm_kmeans(clusters = 12) %>%
  group_by(cluster) %>%
  summarize(sm_paste(document)) %>%
  use_series(document_paste)
```

And once again, you should see some unsurprising (by perhaps still interesting?)
clusters here.

## Word Relationships

In the preceding analyses, we have focused on the analysis of the
document their usage of words. There are often multiple ways of widening a
data set, each leading to different kinds of analysis. The term frequency
data set is no different. We could widen the data set by treating each row as a
term and each column as a document. It is possible to apply dimensionality
reduction and distance metrics on this format as well in order to understand
the relationships between words.

The easiest way to produce a  matrix of the word relationships is by first
using `cnlp_utils_tfidf` as before and then calling the function `t()`
(transpose) to exchange the rows and columns. We will control the maximum
number of features by setting `max_features` to 100 and only considering nouns.
Here is the principal component analysis plot:

```{r, warning=FALSE}
token %>%
  filter(upos == "NOUN") %>%
  cnlp_utils_tfidf(
    min_df = 0, max_df = 1, max_features = 100
  ) %>%
  t() %>%
  sm_tidy_pca(item_name = "word") %>%
  ggplot(aes(x = v1, y = v2)) +
    geom_point(color = "grey90") +
    geom_text_repel(
      aes(label = word),
      show.legend = FALSE
    ) +
    theme_void()
```

As well as the closest pairs of words (here we increase the number of words
to 400):

```{r, warning=FALSE}
token %>%
  filter(upos == "NOUN") %>%
  cnlp_utils_tfidf(
    min_df = 0, max_df = 1, max_features = 400
  ) %>%
  t() %>%
  sm_tidy_angle_distance(item_name = "word") %>%
  filter(word1 < word2) %>%
  group_by(word1) %>%
  arrange(distance) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  arrange(distance)
```

Or even clustering:

```{r, message=FALSE}
set.seed(1)

token %>%
  filter(upos == "NOUN") %>%
  cnlp_utils_tfidf(
    min_df = 0, max_df = 1, max_features = 400
  ) %>%
  t() %>%
  sm_tidy_pca(item_name = "word", n = 10) %>%
  sm_kmeans(clusters = 45, item_name = "word") %>%
  group_by(cluster) %>%
  summarize(words = paste(word, collapse = "; "), n = n()) %>%
  arrange(n) %>%
  use_series(words)
```

Do these relationships seem reasonable to you? Do they tell you anything about
the data or the usage of language within the data that you find surprising?
